# -*- coding: utf-8 -*-
"""Dynamic Fluid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eTiSTmBhGwmgzOJgPnJCnm6UxzrPtZp7

#classification as Laminar, turbulent and preprocess, pattern visualization, correlation and preprocessing
"""

import pandas as pd

try:
    df = pd.read_csv('Flow.csv')
    display(df.head())
    print(df.shape)
except FileNotFoundError:
    print("Error: 'Flow.csv' not found. Please ensure the file exists in the current directory.")
    df = None
except Exception as e:
    print(f"An error occurred: {e}")
    df = None

"""
Explore the dataset by examining its shape, data types, descriptive statistics, target variable distribution, missing values, and correlations between features.

"""

# Examine the shape of the DataFrame
print("Shape of the DataFrame:", df.shape)

# Check data types
print("\nData Types:")
print(df.dtypes)

# Summarize descriptive statistics of numerical features
print("\nDescriptive Statistics:")
print(df.describe())

# Investigate the distribution of the target variable
print("\nDistribution of the target variable:")
print(df['flow_type'].value_counts())
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
df['flow_type'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])
plt.title('Distribution of Flow Types')
plt.xlabel('Flow Type')
plt.ylabel('Count')
plt.show()

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Explore correlations between features, excluding 'flow_type' and 'sample_id'
print("\nCorrelation Matrix (excluding 'flow_type' and 'sample_id'):")
numerical_features = df.select_dtypes(include=['number'])
correlation_matrix = numerical_features.drop('sample_id', axis=1).corr()
display(correlation_matrix)
plt.figure(figsize=(12, 10))
import seaborn as sns
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

# Explore correlations between features, excluding 'flow_type' and 'sample_id'
print("\nCorrelation Matrix (excluding 'flow_type' and 'sample_id'):")
numerical_features = df.select_dtypes(include=['number'])
numerical_features = numerical_features.drop('sample_id', axis=1)
correlation_matrix = numerical_features.corr()
display(correlation_matrix)
plt.figure(figsize=(12, 10))
import seaborn as sns
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Correlation with the target variable
print("\nCorrelation with target variable:")
# Convert 'flow_type' to numerical (0 for laminar, 1 for turbulent) for correlation calculation.
df['flow_type_numeric'] = df['flow_type'].map({'laminar': 0, 'turbulent': 1})
correlations_with_target = numerical_features.corrwith(df['flow_type_numeric'])
print(correlations_with_target)

"""
Identify outliers in the numerical features using box plots and handle them by winsorizing.

"""

import matplotlib.pyplot as plt
import seaborn as sns

# Identify outliers using box plots
numerical_cols = ['t', 'x', 'y', 'u', 'v', 'p', 'dudx', 'dudy', 'dvdx', 'dvdy', 'dudt', 'dvdt']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(y=df[col])
    plt.title(col)
plt.tight_layout()
plt.show()

# Winsorize outliers (capping at 1st and 99th percentiles)
for col in numerical_cols:
    lower_bound = df[col].quantile(0.01)
    upper_bound = df[col].quantile(0.99)
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)

# Re-examine the data after winsorizing
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(y=df[col])
    plt.title(col)
plt.tight_layout()
plt.show()

"""## Data preparation

### Subtask:
Prepare the data for classification by scaling numerical features and encoding the target variable.

**Reasoning**:
Scale numerical features using StandardScaler and encode the target variable 'flow_type'.
"""

from sklearn.preprocessing import StandardScaler

# Assuming 'df' is already loaded and cleaned
numerical_cols = ['t', 'x', 'y', 'u', 'v', 'p', 'dudx', 'dudy', 'dvdx', 'dvdy', 'dudt', 'dvdt']
scaler = StandardScaler()

# Fit and transform the numerical features
df_scaled = df.copy()
df_scaled[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Encode the target variable
df_scaled['flow_type_encoded'] = df_scaled['flow_type'].map({'laminar': 0, 'turbulent': 1})

display(df_scaled.head())

"""
Create visualizations to explore the relationships between features and the target variable 'flow_type', using scatter plots, histograms, box plots, pairplots, and KDE plots.  This will help identify patterns and relationships for distinguishing laminar and turbulent flow.

"""

import matplotlib.pyplot as plt
import seaborn as sns

# Scatter plots for selected feature pairs, colored by flow_type
plt.figure(figsize=(15, 10))
plt.subplot(2, 2, 1)
sns.scatterplot(x='u', y='v', hue='flow_type', data=df_scaled)
plt.title('Velocity Components (u vs v)')

plt.subplot(2, 2, 2)
sns.scatterplot(x='p', y='u', hue='flow_type', data=df_scaled)
plt.title('Pressure vs. Velocity (u)')

plt.subplot(2, 2, 3)
sns.scatterplot(x='dudx', y='dvdy', hue='flow_type', data=df_scaled)
plt.title('Velocity Gradients (dudx vs dvdy)')

plt.subplot(2, 2, 4)
sns.scatterplot(x='dudt', y='dvdt', hue='flow_type', data=df_scaled)
plt.title('Time Derivatives of Velocity (dudt vs dvdt)')
plt.tight_layout()
plt.show()


# Histograms and KDE plots for velocity and pressure
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df_scaled, x='u', hue='flow_type', kde=True, element="step")
plt.title('Velocity (u) Distribution')

plt.subplot(1, 2, 2)
sns.histplot(df_scaled, x='p', hue='flow_type', kde=True, element="step")
plt.title('Pressure (p) Distribution')

plt.tight_layout()
plt.show()

# Box plots for selected features
plt.figure(figsize=(12, 6))
features = ['u', 'v', 'p', 'dudx', 'dudy', 'dvdx', 'dvdy']
for i, col in enumerate(features):
  plt.subplot(2, 4, i + 1)
  sns.boxplot(x='flow_type', y=col, data=df_scaled)
  plt.title(f'{col}')

plt.tight_layout()
plt.show()

# Pairplot for a subset of features
selected_features = ['u', 'v', 'p', 'dudx', 'dudy']
sns.pairplot(df_scaled, vars=selected_features, hue='flow_type', diag_kind="kde")
plt.show()

# Create interaction features
df_scaled['u_times_v'] = df_scaled['u'] * df_scaled['v']
df_scaled['p_div_u'] = df_scaled['p'] / (df_scaled['u'] + 1e-6)
df_scaled['dudx_times_dvdy'] = df_scaled['dudx'] * df_scaled['dvdy']

# Create polynomial features
df_scaled['u_squared'] = df_scaled['u']**2
df_scaled['v_squared'] = df_scaled['v']**2
df_scaled['p_squared'] = df_scaled['p']**2
df_scaled['dudx_squared'] = df_scaled['dudx']**2

# Create ratio features
df_scaled['u_div_v'] = df_scaled['u'] / (df_scaled['v'] + 1e-6)
df_scaled['dudx_div_dvdy'] = df_scaled['dudx'] / (df_scaled['dvdy'] + 1e-6)

# Evaluate the impact of new features
numerical_cols = df_scaled.select_dtypes(include=['number']).columns
correlation_with_target = df_scaled[numerical_cols].corrwith(df_scaled['flow_type_encoded'])
print("Correlation with target variable (flow_type_encoded):\n", correlation_with_target)

# Visualize the impact of new features (example with scatter plots)
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
sns.scatterplot(x='u_times_v', y='flow_type_encoded', hue='flow_type', data=df_scaled)
plt.subplot(1, 3, 2)
sns.scatterplot(x='p_div_u', y='flow_type_encoded', hue='flow_type', data=df_scaled)
plt.subplot(1, 3, 3)
sns.scatterplot(x='dudx_times_dvdy', y='flow_type_encoded', hue='flow_type', data=df_scaled)
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split

# Separate features (X) and target variable (y)
X = df_scaled.drop('flow_type_encoded', axis=1)
y = df_scaled['flow_type_encoded']

# Split data into temporary training and combined validation/testing sets
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Split the combined validation/testing set into separate validation and testing sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

# Verify shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Check class distributions
print("\ny_train distribution:", y_train.value_counts(normalize=True))
print("y_val distribution:", y_val.value_counts(normalize=True))
print("y_test distribution:", y_test.value_counts(normalize=True))

# Drop the 'flow_type' column from X_train and X_val
X_train = X_train.drop('flow_type', axis=1)
X_val = X_val.drop('flow_type', axis=1)

# Initialize the RandomForestClassifier
rf_classifier = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)

# Train the classifier
rf_classifier.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = rf_classifier.predict(X_val)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate evaluation metrics
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
f1 = f1_score(y_val, y_pred)
auc_roc = roc_auc_score(y_val, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# Generate and display the confusion matrix
cm = confusion_matrix(y_val, y_pred)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Laminar', 'Turbulent'],
            yticklabels=['Laminar', 'Turbulent'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

## Feature Importance Analysis

feature_importances = rf_classifier.feature_importances_
feature_names = X_train.columns

plt.figure(figsize=(12, 6))
plt.barh(feature_names, feature_importances)
plt.xlabel("Feature Importance")
plt.ylabel("Feature")
plt.title("Random Forest Feature Importance")
plt.show()

"""#Neural Network"""

# Split the data
X = df_scaled.drop(['flow_type_encoded', 'flow_type'], axis=1)
y = df_scaled['flow_type_encoded']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Build the neural network model
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
    layers.Input(shape=(X_train.shape[1],)),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Make predictions on the validation set
y_pred_probs = model.predict(X_val)
y_pred = (y_pred_probs > 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
f1 = f1_score(y_val, y_pred)
auc_roc = roc_auc_score(y_val, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# Display the confusion matrix
cm = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Laminar', 'Turbulent'],
            yticklabels=['Laminar', 'Turbulent'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import cross_val_score
import numpy as np

# Calculate MAE
mae = mean_absolute_error(y_val, y_pred)  # Mean Absolute Error
print(f"MAE: {mae:.4f}")

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_val, y_pred))  # Root Mean Squared Error
print(f"RMSE: {rmse:.4f}")

# Cross-validation (example with 5 folds)
cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")
print(f"Average cross-validation score: {np.mean(cv_scores):.4f}")

